<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Unleashing Generalization of End-to-End Autonomous Driving with Controllable Long Video Generation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Unleashing Generalization of End-to-End Autonomous Driving with Controllable Long Video Generation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><a>Enhui Ma</a><sup>7*†‡</sup>,</span>
              <span class="author-block"><a>Lijun Zhou</a><sup>2†</sup>,</span>
              <span class="author-block"><a>Tao Tang</a><sup>3*</sup>,</span> 
              <span class="author-block"><a>Zhan Zhang</a><sup>4‡</sup>,</span> 
              <span class="author-block"><a>Dong Han</a><sup>5‡</sup>,</span> 
              <span class="author-block"><a>Junpeng Jiang</a><sup>6*</sup>,</span> 
              <span class="author-block"><a>Kun Zhan</a><sup>2</sup>,</span> 
              <span class="author-block"><a>Peng Jia</a><sup>2</sup>,</span> 
              <span class="author-block"><a>Xianpeng Lang</a><sup>2</sup>,</span> 
              <span class="author-block"><a>Haiyang Sun</a><sup>2</sup>,</span> 
              <span class="author-block"><a>Di Lin</a><sup>7</sup>,</span> 
              <span class="author-block"><a>Kaicheng Yu</a><sup>1§</sup></span> 
            </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Westlake University,<br></span>
                    <span class="author-block"><sup>2</sup>Li Auto Inc.,<br></span>
                    <span class="author-block"><sup>3</sup>Shenzhen Campus, Sun Yat-sen University,<br></span>
                    <span class="author-block"><sup>4</sup>Southeast University,<br></span>
                    <span class="author-block"><sup>5</sup>Harbin Engineering University,<br></span>
                    <span class="author-block"><sup>6</sup>Harbin Institute of Technology(Shenzhen),<br></span>
                    <span class="author-block"><sup>7</sup>Tianjin University<br></span>
                    <span class="eql-cntrb"><br><small>(<sup>*</sup>Work during an internship at Li Auto Inc. <sup>†</sup>Co-first authors. <sup>‡</sup>Work during a visit to Westlake University. <sup>§</sup>Corresponding authors.)</small> </span>
                  </div>

                  <!-- <div class="column has-text-centered">
                    <div class="publication-links"> -->
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            <!-- </div>
          </div> -->
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <!-- <div class="hero-body"> -->
    <div class="item">
      <img src="static/images/teaser.png" alt="MY ALT TEXT" />
      <h2 style="font-size: 18px" class="subtitle has-text-centered">
        <b>Overview of our method.
          <!-- <br> -->
        </b> We show that <b>(a)</b> our <i>Delphi</i> can generate up to 40 frames consecutive videos while <b>(b)</b> existing best only generate 8 frames. <b>(c)</b> With the failure-cased driven framework equipped with <i>Delphi</i>, <b>(d)</b> we can significantly boost the end-to-end model performance with much smaller cost.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Using generative models to synthesize new data has become a de-facto standard in autonomous driving to address the data scarcity issue. Though existing approaches are able to boost perception models, we discover that these approaches fail to improve the performance of planning of end-to-end autonomous driving models as the generated videos are usually less than 8 frames and the spatial and temporal inconsistencies are not negligible. To this end, we propose <i>Delphi</i>, a novel diffusion-based long video generation method with a shared noise modeling mechanism across the multi-views to increase spatial consistency, and a feature-aligned module to achieves both precise controllability and temporal consistency. Our method can generate up to 40 frames of video without loss of consistency which is about 5 times longer compared with state-of-the-art methods. Instead of randomly generating new data, we further design a sampling policy to let <i>Delphi</i> generate new data that are similar to those failure cases to improve the sample efficiency. This is achieved by building a failure-case driven framework with the help of pre-trained visual language models. Our extensive experiment demonstrates that our <i>Delphi</i> generates a higher quality of long videos surpassing previous state-of-the-art methods. Consequentially, with only generating 4% of the training dataset size, our framework is able to go beyond perception and prediction task, for the first time to the best of our knowledge, boost the planning performance of the end-to-end autonomous driving model by a margin of 25%.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/delphi-framework-small.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Overview of <i>Delphi</i>.
          <!-- <b>(a) Architecture of <i>Delphi</i></b>.  -->
          <!-- It takes multi-view videos z and the corresponding BEV (Bird's Eye View) layout sequences as input. Each video consists of N frames and V views. The BEV layout sequences are first projected into camera space according to camera parameters, resulting in camera layouts that include both foreground and background layouts. 
          Specifically, the foreground layout includes the bounding box's corner coordinates, heading, instance id, and dense caption, while the background one includes different colored lines to represent road trends. 
          The layout embeddings, processed by the encoder, are injected into the U-Net through cross-attention to achieve fine-grained layout control in the generation process. Additionally, we leverage VLM to extract dense captions for the input scenes,  which are encoded by Long-CLIP to obtain text embeddings, which are then injected into the U-Net via text cross-attention to achieve text-based control. 
          We further design two key modules,  
          <b>(b) Noise Reinitialization Module</b> that encompass a share noise across different views  and  <b>(c) Feature-aligned Temporal Consistency Module</b> to ensure spatial and temporal consistency accordingly. -->
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/failure-case-driven framwork.jpeg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Overview of Failure-case Driven Framework.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Long video generation on nuScenes dataset</h2>

      <div class="content has-text-justified">
        <p>
          Long videos generated by Delphi (up to 40 frames) on the nuScenes dataset. For readability, we play the video at 5x speed.
        </p>
      </div>
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/speed5_videos/file_v3_00bd_5363e497-7f41-4bfd-9330-cf9a65b2c9dg.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/speed5_videos/modified_200_48ba943c3d19463a81281bf6a7078eac.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/speed5_videos/modified_200_d6e6f5d622474271b695f4c7f07df7bb.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video4">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/speed5_videos/modified_200_e8bda599a2d44d76a7692caeacb1ba6e.mp4"
            type="video/mp4">
          </video>
        </div>
      <!-- </div> -->

      <!-- <div class="content has-text-justified">
        <p>
          We compare the long video generation results with the state-of-the-art methods on the nuScenes dataset.
        </p>
      </div> -->

      <!-- <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/cmp1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/cmp2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">
            <source src="static/videos/cmp3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div> -->
    </div>
  </div>
</section>
<!-- End video carousel -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Precise controllability</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/results/visual conparison of local region.png" alt="MY ALT TEXT"/>
            <h2 class="subtitle has-text-centered">
              Visual comparison of local region generated by different generative models. Our method maintains consistent spatial and temporal appearance where the previous methods fail.
            </h2>
          </div>

          <div class="item">
            <!-- Your image here -->
            <img src="static/images/results/visual precise control.jpeg" alt="MY ALT TEXT"/>
            <h2 class="subtitle has-text-centered">
              Visualization of <b>(a)</b> instance-level editing, including appearance attributes of all vehicles, and  <b>(b)</b> scene-level editing, including weather and time.
            </h2>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Our failure-case driven framework boosts the end-to-end planning model</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/downstream/Tab2.jpeg" alt="MY ALT TEXT"/>
            <h2 class="subtitle has-text-centered">
              Performance comparison of the end-to-end models fine-tuned from the UniAD open source model by applying different data sampling strategies, numbers of data cases, data engines, and data sources in the failure-case driven framework.The baseline performance is presented in the first row of the table.
            </h2>
          </div>
          
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/downstream/uniad generalization comparison.png" alt="MY ALT TEXT"/>
            <h2 class="subtitle has-text-centered">
              Visualization of four examples before and after. <b>(a)</b> Here, we show four hard examples from the validation set, ''large objects in the front'' and ''unprotected left turn at intersection''. <b>(b)</b> Our framework is able to fix these four examples without using these data during training. 
            </h2>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Scaling up</h2>

      <div class="content has-text-justified">
        <p>
          By training on a private multi-view driving dataset &copy (the training data is about 50 times larger than nuScenes), <i>Delphi</i> demonstrates interesting capabilities to generate up to 120 frames of spatiotemporally consistent videos. This fully demonstrates the scalability of our <i>Delphi</i>.
        </p>
      </div>

      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item item-video1">
          <video poster="" id="video1" playsinline autoplay controls muted loop height="100%">
            <source src="static/videos/liauto_1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" playsinline autoplay controls muted loop height="100%">
            <source src="static/videos/liauto_2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" playsinline autoplay controls muted loop height="100%">
            <source src="static/videos/liauto_3.mp4"
            type="video/mp4">
          </video>
        </div>
      <!-- </div> -->
      <footer>
        <p>&copy; Li Auto Inc. All rights reserved.</p>
      </footer>
    </div>
  </div>
</section>
<!-- End video carousel -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Application: visual renderer for closed-loop evaulation</h2>

      <div class="content has-text-justified">
        <p>
          We note that <i>Delphi</i> can also be used as a data engine with photorealistic image generation capabilities and further supports closed-loop evaluation of end-to-end models such as UniAD. Here we show a demo of closed-loop evaluation on nuNcenes. The top row shows an open-loop evaluation scenario on nuNcenes: the ego car is driving at a constant speed. The bottom row shows a closed-loop evaluation scenario on nuNcenes using Delphi: the ego car is accelerating, resulting in a dangerous distance from the vehicle in front.
        </p>
      </div>
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/close_loop.mp4"
            type="video/mp4">
          </video>
        </div>
      <!-- </div> -->

    </div>
  </div>
</section>
<!-- End video carousel -->



<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <!-- <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer> -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
